
---
使用filebeat收集日志，然后发送给logstash进行filter数据过滤，最后保存到es里面
---

## 文件输出

```
output:
  file:
    path: "/tmp/filebeat-test"
    filename: log-test.log
    rotate_every_kb: 1000
    number_of_files: 7

# 每个文件最大1000b（1kb），然后最多保留7个这样的文件

```


##


filebeat.prospectors:
- type: log
  enabled: true
  paths:
    - /data/Application/qdp-oasis-web/logs/*

  fields:
    type: "oasis"

> paths 也可以写多个路径文件
> fields 可以添加自定义的附加字段，用于区分各种数据  type可以修改，systype


## 直接输出到es

```
#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["10.37.251.67:9200"]
```

##

---
tail_files:
false

可以配置为true和false。配置为true时，filebeat将从新文件的最后位置开始读取，如果配合日志轮循使用，新文件的第一行将被跳过

tail_files：如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送，而不是从文件开始处重新发送所有内容。
---

## 安装logstash

新建logstash.conf 文件，内容为：

input {
  stdin { }
  beats {
    port => 5044
  }
}
output {
  elasticsearch {
    hosts => ["localhost:9200"]
  }
  stdout { codec => rubydebug }
}


后台运行:

nohup bin/logstash -f logstash.conf &

































































































































































































































































































































































































































































































































































































































































































































































---
